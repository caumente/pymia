{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% raw\n"
    },
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    ".. _example-data2:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Data extraction and assembly\n",
    "\n",
    "This example shows how to use the `pymia.data` package to extract chunks of data from the dataset and to assemble the chunks\n",
    "to feed a deep neural network. It also shows how the predicted chunks are assembled back to full-images predictions.\n",
    "\n",
    "The extraction-assemble principle is essential for large three-dimensional images that do not fit entirely in the GPU memory\n",
    "and thus require some kind of patch-based approach.\n",
    "\n",
    "For simplicity reasons we use slice-wise extraction in this example, meaning that the two-dimensional slices are extracted\n",
    "from the three-dimensional image.\n",
    "\n",
    "The example uses PyTorch as a deep learning (DL) framework. The minimal adaptions needed for TensorFlow are shown at the end.\n",
    "\n",
    "The Jupyter notebook can be found at [./examples/data/extraction_assembling.ipynb](https://github.com/rundherum/pymia/blob/master/examples/data/extraction_assembling.ipynb).\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "Note\n",
    "\n",
    "To be able to run this example:\n",
    "\n",
    "- Get the example data by executing [./examples/example-data/pull_example_data.py](https://github.com/rundherum/pymia/blob/master/examples/example-data/pull_example_data.py).\n",
    "- Install torch (`pip install torch`).\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Import the required modules.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pymia.data.assembler as assm\n",
    "import pymia.data.transformation as tfm\n",
    "import pymia.data.definition as defs\n",
    "import pymia.data.extraction as extr\n",
    "import pymia.data.backends.pytorch as pymia_torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we create the the access to the .h5 dataset by defining: (i) the indexing strategy (`indexing_strategy`)\n",
    "that defines the chunks of data to be retrieved, (ii) the information to be extracted (`extractor`), and (iii)\n",
    "the transformation (`transform`) to be applied after extraction.\n",
    "\n",
    "The permutation transform is required since the channels (here _T1_, _T2_) are stored in the last dimension in the .h5 dataset\n",
    "but PyTorch requires channel-first format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "hdf_file = '../example-data/example-dataset.h5'\n",
    "\n",
    "# Data extractor for extracting the \"images\" entries\n",
    "extractor = extr.DataExtractor(categories=(defs.KEY_IMAGES,))\n",
    "# Permutation transform to go from HWC to CHW.\n",
    "transform = tfm.Permute(permutation=(2, 0, 1), entries=(defs.KEY_IMAGES,))\n",
    "# Indexing defining a slice-wise extraction of the data\n",
    "indexing_strategy = extr.SliceIndexing()\n",
    "\n",
    "dataset = extr.PymiaDatasource(hdf_file, indexing_strategy, extractor, transform)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define an assembler that will puts the data/image chunks back together after prediction of the input chunks. This is\n",
    "required to perform a evaluation on entire subjects, and any further processing such as saving the predictions.\n",
    "\n",
    "Also, we define extractors that we will use to extract information required after prediction. This information not need\n",
    "to be chunked (/indexed/sliced) and not need to interact with the DL framework. Thus, it can be extracted\n",
    "directly form the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "assembler = assm.SubjectAssembler(dataset)\n",
    "\n",
    "direct_extractor = extr.ComposeExtractor([\n",
    "    extr.ImagePropertiesExtractor(),  # Extraction of image properties (origin, spacing, etc.) for storage\n",
    "    extr.DataExtractor(categories=(defs.KEY_LABELS,))  # Extraction of \"labels\" entries for evaluation\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PyTorch-specific**\n",
    "The loop over the batches and the neural network architecture are framework dependent. We show here the PyTorch case, but the\n",
    "TensorFlow equivalent can be found at the end of this notebook.\n",
    "\n",
    "Basically, all we have to do is to wrap our dataset as PyTorch dataset, to build a PyTorch data loader, and to create/load a\n",
    "network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as torch_data\n",
    "\n",
    "# Wrap the pymia datasource\n",
    "pytorch_dataset = pymia_torch.PytorchDatasetAdapter(dataset)\n",
    "loader = torch_data.dataloader.DataLoader(pytorch_dataset, batch_size=2, shuffle=False)\n",
    "\n",
    "# Dummy network representing a placeholder for a trained network\n",
    "dummy_network = nn.Sequential(\n",
    "    nn.Conv2d(in_channels=2, out_channels=8, kernel_size=3, padding=1),\n",
    "    nn.Conv2d(in_channels=8, out_channels=1, kernel_size=3, padding=1),\n",
    "    nn.Sigmoid()\n",
    ").eval()\n",
    "torch.set_grad_enabled(False)  # no gradients needed for testing\n",
    "\n",
    "nb_batches = len(loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to loop over batches of data chunks. After the usual prediction of the network, the predicted data is\n",
    "provided to the assembler, which takes care of putting chunks back together. Once some subjects are assembled\n",
    "(`subjects_ready`) we extract the data required for evaluation and storing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for i, batch in enumerate(loader):\n",
    "\n",
    "    # Get data from batch and predict\n",
    "    x, sample_indices = batch[defs.KEY_IMAGES], batch[defs.KEY_SAMPLE_INDEX]\n",
    "    prediction = dummy_network(x)\n",
    "\n",
    "    # translate the prediction to numpy and back to (B)HWC (channel last)\n",
    "    numpy_prediction = prediction.numpy().transpose((0, 2, 3, 1))\n",
    "\n",
    "    # add the batch prediction to the assembler\n",
    "    is_last = i == nb_batches - 1\n",
    "    assembler.add_batch(numpy_prediction, sample_indices.numpy(), is_last)\n",
    "\n",
    "    # Process the subjects/images that are fully assembled\n",
    "    for subject_index in assembler.subjects_ready:\n",
    "        subject_prediction = assembler.get_assembled_subject(subject_index)\n",
    "\n",
    "        # Extract the target and image properties via direct extract\n",
    "        direct_sample = dataset.direct_extract(direct_extractor, subject_index)\n",
    "        target, image_properties = direct_sample[defs.KEY_LABELS], direct_sample[defs.KEY_PROPERTIES]\n",
    "\n",
    "        # # Do whatever you desire...\n",
    "        # do_eval()\n",
    "        # do_save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TensorFlow adaptions**\n",
    "For the presented data handling to work with the TensorFlow framework, only minor modifications are required:\n",
    "(1) Modifications in the input, (2) different framework-specific code, and (3) no permutations required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "```python\n",
    "# 1)\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow.keras.layers as layers\n",
    "import pymia.data.backends.tensorflow as pymia_tf\n",
    "\n",
    "# 2)\n",
    "gen_fn = pymia_tf.get_tf_generator(dataset)\n",
    "tf_dataset = tf.data.Dataset.from_generator(generator=gen_fn,\n",
    "                                            output_types={defs.KEY_IMAGES: tf.float32,\n",
    "                                                          defs.KEY_SAMPLE_INDEX: tf.int64})\n",
    "loader = tf_dataset.batch(2)\n",
    "\n",
    "dummy_network = keras.Sequential([\n",
    "    layers.Conv2D(8, kernel_size=3, padding='same'),\n",
    "    layers.Conv2D(2, kernel_size=3, padding='same', activation='sigmoid')]\n",
    ")\n",
    "nb_batches = len(dataset) // 2\n",
    "\n",
    "# 3)\n",
    "# No permutation transform needed. Thus the lines\n",
    "transform = tfm.Permute(permutation=(2, 0, 1), entries=(defs.KEY_IMAGES,))\n",
    "numpy_prediction = prediction.numpy().transpose((0, 2, 3, 1))\n",
    "# become\n",
    "transform = None\n",
    "numpy_prediction = prediction.numpy()\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}